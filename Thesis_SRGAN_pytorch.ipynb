{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45006faaf2f54bea89e00d471af013e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3e7f37c10fb41e68d2b3e90602e308f",
              "IPY_MODEL_7608f9ca3eea4d7ab7b099c1a407817f",
              "IPY_MODEL_f8c956ae5675444ca2416859adbd3363"
            ],
            "layout": "IPY_MODEL_4147a551d9464f75b0710beffe41611d"
          }
        },
        "c3e7f37c10fb41e68d2b3e90602e308f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbec275049f845d089e780b99ec66a3d",
            "placeholder": "​",
            "style": "IPY_MODEL_d24a08ed8a1e47b6b92f576bc692912d",
            "value": "100%"
          }
        },
        "7608f9ca3eea4d7ab7b099c1a407817f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_552ce009f3d4452182879319d9438c88",
            "max": 574673361,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2af9daa2df8b4856ba6f8e4d6d875bb6",
            "value": 574673361
          }
        },
        "f8c956ae5675444ca2416859adbd3363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89517d8beac945218b9fd00b0fb28af4",
            "placeholder": "​",
            "style": "IPY_MODEL_d2e461bd2a2440a58401180bc2e7e5ee",
            "value": " 548M/548M [00:04&lt;00:00, 111MB/s]"
          }
        },
        "4147a551d9464f75b0710beffe41611d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbec275049f845d089e780b99ec66a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24a08ed8a1e47b6b92f576bc692912d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "552ce009f3d4452182879319d9438c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2af9daa2df8b4856ba6f8e4d6d875bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89517d8beac945218b9fd00b0fb28af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2e461bd2a2440a58401180bc2e7e5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ2XZilVc03m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        discriminator=False,\n",
        "        use_act=True,\n",
        "        use_bn=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_act = use_act\n",
        "        self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias=not use_bn)\n",
        "        self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "        self.act = (\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "            if discriminator\n",
        "            else nn.PReLU(num_parameters=out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.cnn(x))) if self.use_act else self.bn(self.cnn(x))\n"
      ],
      "metadata": {
        "id": "C41he0NsdUUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "    self.block1 = ConvBlock(\n",
        "        in_channels,\n",
        "        in_channels,\n",
        "        kernel_size = 3,\n",
        "        stride =1 ,\n",
        "        padding = 1, \n",
        "        \n",
        "    )\n",
        "    self.block2 = ConvBlock(\n",
        "        in_channels,\n",
        "\n",
        "        in_channels,\n",
        "        kernel_size = 3,\n",
        "        stride =1 ,\n",
        "        padding = 1, \n",
        "        use_act = True\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.block1(x)\n",
        "    out = self.block2(out)\n",
        "    return out+x  "
      ],
      "metadata": {
        "id": "FfG5pugwdYCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DeconvulationBlock(nn.Module):\n",
        "    def __init__(self,  in_channels, shape_1, shape_2):\n",
        "        super().__init__()\n",
        "        self.conv = ConvBlock(\n",
        "            in_channels*2,\n",
        "            in_channels,\n",
        "            kernel_size = 3,\n",
        "            stride =1 ,\n",
        "            padding = 1\n",
        "\n",
        "\n",
        "        )\n",
        "        self.shape_1 = shape_1\n",
        "        self.shape_2 = shape_2\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = torch.nn.functional.interpolate(x,size=(self.shape_1,self.shape_2), mode='bilinear')\n",
        "      return self.conv(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "eyUnEMF6dbqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decon = DeconvulationBlock(64, 128, 128)"
      ],
      "metadata": {
        "id": "qnB5gbg4deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.randn(1,3,64,64)"
      ],
      "metadata": {
        "id": "k19oV4O5eETt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_channels=32, num_blocks=1, image_size1=256,image_size2=256):\n",
        "        super().__init__()\n",
        "        self.initial = ConvBlock(in_channels, num_channels, kernel_size=9, stride=1, padding=4, use_bn=True)\n",
        "        self.conv1 = ConvBlock(num_channels,num_channels*2,kernel_size=3,stride=1)\n",
        "        self.conv2 = ConvBlock(num_channels*2,num_channels*4,kernel_size=3,stride=1)\n",
        "        self.residuals1 = nn.Sequential(*[ResidualBlock(num_channels*4) for _ in range(num_blocks)])\n",
        "        self.residuals2 = nn.Sequential(*[ResidualBlock(num_channels*4) for _ in range(num_blocks)])\n",
        "        self.residuals3 = nn.Sequential(*[ResidualBlock(num_channels*4) for _ in range(num_blocks)])\n",
        "        self.deconconvblock1 = DeconvulationBlock(num_channels*2, num_channels*4, num_channels*4)\n",
        "        self.deconconvblock2 = DeconvulationBlock(32, image_size1, image_size2 )\n",
        "        self.final = nn.Conv2d(32, in_channels, kernel_size=9, stride=1,padding =4)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        input = x\n",
        "        initial = self.initial(x)\n",
        "        print(initial.shape)\n",
        "        x = self.conv1(initial)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.residuals1(x)\n",
        "        x = self.residuals2(x)\n",
        "        x = self.residuals3(x)\n",
        "        \n",
        "\n",
        "        x = self.deconconvblock1(x)\n",
        "        x = self.deconconvblock2(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x = x+initial\n",
        "        print(x.shape)\n",
        "\n",
        "        x = self.final(x)\n",
        "\n",
        "        print(x.shape)\n",
        "\n",
        "        x = x+input\n",
        "        \n",
        "\n",
        "        return torch.tanh(x)\n"
      ],
      "metadata": {
        "id": "YwZI9wR0eGzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        low_resolution = 24  # 96x96 -> 24x24\n",
        "        x = torch.randn((1,3,256,256))\n",
        "        gen =  Generator()\n",
        "        gen_out = gen(x)\n",
        "        # disc = Discriminator()\n",
        "        # disc_out = disc(gen_out)\n",
        "\n",
        "        print(gen_out.shape)\n",
        "        # print(disc_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YgCo6lbeKpU",
        "outputId": "f1ee76a1-ed21-4879-9209-b9314b672f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.flatten import Flatten\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,  in_channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n",
        "      super().__init__()\n",
        "      blocks= []\n",
        "      for idx,feature in enumerate(features):\n",
        "        blocks.append(\n",
        "            ConvBlock(\n",
        "                in_channels,\n",
        "                feature,\n",
        "                kernel_size = 3,\n",
        "                stride =1 +idx%2,\n",
        "                padding = 1,\n",
        "                discriminator= True,\n",
        "                use_act = True,\n",
        "                use_bn = False if idx ==0 else True,\n",
        "\n",
        "            )\n",
        "        )\n",
        "        in_channels = feature\n",
        "\n",
        "      self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.AdaptiveAvgPool2d((6,6)),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(512*6*6, 1024),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Linear(1024,1)\n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.blocks(x)\n",
        "    return self.classifier(x)"
      ],
      "metadata": {
        "id": "iBFymP3Iene0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc = Discriminator()"
      ],
      "metadata": {
        "id": "UhPbRN-HezBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disc_out= disc(gen_out)"
      ],
      "metadata": {
        "id": "JZhtC2Ffe3c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(disc_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PsAIS8We5SU",
        "outputId": "111c8ad4-14e4-4ffb-f2c5-cc750f525b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import vgg19\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Ss1B9IM6e7QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oDNTL5whe-Vs",
        "outputId": "5d8f6f55-325e-4900-f8ff-5ad1d975d778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGLoss(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      self.vgg = vgg19(pretrained=True).features[:36].eval().to(DEVICE)\n",
        "\n",
        "      self.loss = nn.MSELoss()\n",
        "\n",
        "\n",
        "      for param in self.vgg.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    vgg_input_features = self.vgg(input)\n",
        "    vgg_target_features = self.vgg(target)\n",
        "    return self.loss(vgg_input_features, vgg_input_features)"
      ],
      "metadata": {
        "id": "XTmhas1WfA3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_loss = VGGLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "45006faaf2f54bea89e00d471af013e3",
            "c3e7f37c10fb41e68d2b3e90602e308f",
            "7608f9ca3eea4d7ab7b099c1a407817f",
            "f8c956ae5675444ca2416859adbd3363",
            "4147a551d9464f75b0710beffe41611d",
            "bbec275049f845d089e780b99ec66a3d",
            "d24a08ed8a1e47b6b92f576bc692912d",
            "552ce009f3d4452182879319d9438c88",
            "2af9daa2df8b4856ba6f8e4d6d875bb6",
            "89517d8beac945218b9fd00b0fb28af4",
            "d2e461bd2a2440a58401180bc2e7e5ee"
          ]
        },
        "id": "B6xeNNELfEbN",
        "outputId": "ca981471-bb5f-4730-8354-cada467b8cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45006faaf2f54bea89e00d471af013e3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "unCQR_LyfGX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenoiseDataset(Dataset):\n",
        "    def __init__(self, train_noise, train_real, transform=None):\n",
        "        self.train_noise = train_noise\n",
        "        self.train_real = train_real\n",
        "        self.transform = transform\n",
        "\n",
        "        self.train_noise_images = os.listdir(train_noise)\n",
        "        self.train_real_images = os.listdir(train_real)\n",
        "        self.length_dataset = max(len(self.train_noise_images), len(self.train_real_images)) # 1000, 1500\n",
        "        self.train_noise_len = len(self.train_noise_images)\n",
        "        self.train_real_len = len(self.train_real_images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length_dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        train_noise_img = self.train_noise_images[index % self.train_noise_len]\n",
        "        train_real_img = self.train_real_images[index % self.train_real_len]\n",
        "\n",
        "        train_noise_path = os.path.join(self.train_noise, train_noise_img)\n",
        "        train_real_path = os.path.join(self.train_real, train_real_img)\n",
        "\n",
        "        train_noise_img = np.array(Image.open(train_noise_path).convert(\"RGB\"))\n",
        "        train_real_img = np.array(Image.open(train_real_path).convert(\"RGB\"))\n",
        "\n",
        "        if self.transform:\n",
        "            augmentations = self.transform(image=train_noise_img, image0=train_real_img)\n",
        "            train_noise_img = augmentations[\"image\"]\n",
        "            train_real_img = augmentations[\"image0\"]\n",
        "\n",
        "        return train_noise_img, train_real_img"
      ],
      "metadata": {
        "id": "1m9GpfkAfU9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations as A"
      ],
      "metadata": {
        "id": "HzkS7EMdjtHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=256, height=256),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
        "        ToTensorV2(),\n",
        "     ],\n",
        "    additional_targets={\"image0\": \"image\"},\n",
        ")"
      ],
      "metadata": {
        "id": "UYlANuFHjjwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "QQw8vb9gmMs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    dataset = DenoiseDataset(\n",
        "      train_noise = '/content/drive/MyDrive/train/train', train_real='/content/drive/MyDrive/train_cleaned/train_cleaned', transform=\n",
        "transforms\n",
        "    )\n",
        "    loader = DataLoader(dataset, batch_size=1, num_workers=4)\n",
        "\n",
        "    for low_res, high_res in loader:\n",
        "        print(low_res.shape)\n",
        "        print(high_res.shape)"
      ],
      "metadata": {
        "id": "3rnc93o5jkfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/drive/MyDrive/train')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u22M7mtInP62",
        "outputId": "8cd50485-571d-4a9d-bc03-dd30e95b62fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "UNVZO1lWn5Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic, real, fake, device):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake.detach() * (1 - alpha)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n"
      ],
      "metadata": {
        "id": "TIfA_ZrepHqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n"
      ],
      "metadata": {
        "id": "LiOXO4UmpKWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n"
      ],
      "metadata": {
        "id": "DOXLhkG6pMpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_examples(low_res_folder, gen):\n",
        "    files = os.listdir(low_res_folder)\n",
        "\n",
        "    gen.eval()\n",
        "    for file in files:\n",
        "        image = Image.open(\"/content/drive/MyDrive/test/\" + file)\n",
        "        with torch.no_grad():\n",
        "            upscaled_img = gen(\n",
        "                test_transform(image=np.asarray(image))[\"image\"]\n",
        "                .unsqueeze(0)\n",
        "                .to(DEVICE)\n",
        "            )\n",
        "        save_image(upscaled_img * 0.5 + 0.5, f\"saved/{file}\")\n",
        "    gen.train()"
      ],
      "metadata": {
        "id": "cYyloEWrpOzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "uK8QBGLKpWBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_MODEL = True\n",
        "SAVE_MODEL = True\n",
        "CHECKPOINT_GEN = \"/content/drive/MyDrive/gen.pth.tar\"\n",
        "CHECKPOINT_DISC = \"/content/drive/MyDrive/disc.pth.tar\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 4\n",
        "HIGH_RES = 96\n",
        "LOW_RES = HIGH_RES // 4\n",
        "IMG_CHANNELS = 3"
      ],
      "metadata": {
        "id": "8lv1wWojpd3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch import optim\n"
      ],
      "metadata": {
        "id": "-t4kY3HupvHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg_loss):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for idx, (low_res, high_res) in enumerate(loop):\n",
        "        high_res = high_res.to(DEVICE)\n",
        "        low_res = low_res.to(DEVICE)\n",
        "        \n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        fake = gen(low_res)\n",
        "        disc_real = disc(high_res)\n",
        "        disc_fake = disc(fake.detach())\n",
        "        disc_loss_real = bce(\n",
        "            disc_real, torch.ones_like(disc_real) - 0.1 * torch.rand_like(disc_real)\n",
        "        )\n",
        "        disc_loss_fake = bce(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = disc_loss_fake + disc_loss_real\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        disc_fake = disc(fake)\n",
        "        #l2_loss = mse(fake, high_res)\n",
        "        adversarial_loss = 1e-3 * bce(disc_fake, torch.ones_like(disc_fake))\n",
        "        loss_for_vgg = 0.006 * vgg_loss(fake, high_res)\n",
        "        gen_loss = loss_for_vgg + adversarial_loss\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        if idx % 200 == 0:\n",
        "            plot_examples(\"/content/drive/MyDrive/test\", gen)\n",
        "\n"
      ],
      "metadata": {
        "id": "1PzmkujbphP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "   dataset = DenoiseDataset(\n",
        "      train_noise = '/content/drive/MyDrive/train/train', train_real='/content/drive/MyDrive/train_cleaned/train_cleaned', transform=\n",
        "transforms\n",
        "    )\n",
        "   loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "   gen = Generator(in_channels=3).to(DEVICE)\n",
        "   disc = Discriminator(in_channels=3).to(DEVICE)\n",
        "   opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "   opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "   mse = nn.MSELoss()\n",
        "   bce = nn.BCEWithLogitsLoss()\n",
        "   vgg_loss = VGGLoss()\n",
        "\n",
        "  #  if LOAD_MODEL:\n",
        "  #       load_checkpoint(\n",
        "  #           CHECKPOINT_GEN,\n",
        "  #           gen,\n",
        "  #           opt_gen,\n",
        "  #           LEARNING_RATE,\n",
        "  #       )\n",
        "  #       load_checkpoint(\n",
        "  #          CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE,\n",
        "  #       )\n",
        "\n",
        "   for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg_loss)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
        "            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)"
      ],
      "metadata": {
        "id": "Hdx5wyB9p3Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "By7fZu6crPeY",
        "outputId": "734054e8-8992-40e0-e206-a8dffa5ee501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/144 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 32, 256, 256])\n",
            "torch.Size([1, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/144 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-4c995e7e068d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-d971185b9c05>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSAVE_MODEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-e4496386e719>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(loader, disc, gen, opt_gen, opt_disc, mse, bce, vgg_loss)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mplot_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-522fc7e9b986>\u001b[0m in \u001b[0;36mplot_examples\u001b[0;34m(low_res_folder, gen)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             upscaled_img = gen(\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mtest_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_each_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     )\n\u001b[1;32m     97\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, image, **params)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pixel_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_transform_init_args_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(img, mean, std, max_pixel_value)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (420,540) (3,) (420,540) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os.path\n",
        "\n",
        "def make_tarfile(output_filename, source_dir):\n",
        "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
      ],
      "metadata": {
        "id": "80C_lEyGrV2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_tarfile( 'gen.pth', '/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "NNStaFkVtyyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_tarfile( 'disc.pth', '/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "bQckFT16t-Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "oyZIMD2GucU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del variables\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "fBHWohxJ27aA",
        "outputId": "aba1b512-5a43-4324-f5d4-e6d923c989ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-4bb94bc31a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'variables' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "6HsI03Es3DAY",
        "outputId": "48c70888-fea1-487b-d400-d7b01b027919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 7            |        cudaMalloc retries: 7         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   12536 MB |   13205 MB |   41366 MB |   28829 MB |\\n|       from large pool |   12488 MB |   13150 MB |   41294 MB |   28806 MB |\\n|       from small pool |      48 MB |      54 MB |      71 MB |      22 MB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   12536 MB |   13205 MB |   41366 MB |   28829 MB |\\n|       from large pool |   12488 MB |   13150 MB |   41294 MB |   28806 MB |\\n|       from small pool |      48 MB |      54 MB |      71 MB |      22 MB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   13700 MB |   13834 MB |   13834 MB |  137216 KB |\\n|       from large pool |   13646 MB |   13778 MB |   13778 MB |  135168 KB |\\n|       from small pool |      54 MB |      56 MB |      56 MB |    2048 KB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |    1163 MB |    9115 MB |   25029 MB |   23866 MB |\\n|       from large pool |    1157 MB |    9110 MB |   24959 MB |   23801 MB |\\n|       from small pool |       5 MB |       7 MB |      69 MB |      64 MB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1275    |    1437    |    1821    |     546    |\\n|       from large pool |     164    |     182    |     245    |      81    |\\n|       from small pool |    1111    |    1255    |    1576    |     465    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1275    |    1437    |    1821    |     546    |\\n|       from large pool |     164    |     182    |     245    |      81    |\\n|       from small pool |    1111    |    1255    |    1576    |     465    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      77    |      82    |      82    |       5    |\\n|       from large pool |      50    |      54    |      54    |       4    |\\n|       from small pool |      27    |      28    |      28    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      38    |      55    |     184    |     146    |\\n|       from large pool |      31    |      38    |      88    |      57    |\\n|       from small pool |       7    |      23    |      96    |      89    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3WOnabF3Msh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}